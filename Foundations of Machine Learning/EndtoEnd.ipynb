{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# End-to-End Machine Learning\n",
    "\n",
    "## Author: AC\n",
    "\n",
    "## Date: 27-11-2025\n",
    "\n",
    "### Learning Outcomes:\n",
    "1. Master the End-to-End Machine Learning Workflow\n",
    "\n",
    "- Execute a complete machine learning project loop: from data acquisition, cleaning, and feature engineering to model training and evaluation.\n",
    "\n",
    "- Understand how to structure a data processing workflow that meets industry standards.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Perform Advanced Data Preprocessing\n",
    "\n",
    "- Missing Values: Use SimpleImputer for median or mode imputation instead of dropping data.\n",
    "\n",
    "- Outlier Detection: Apply various denoising strategies, including IQR, Winsorization (Capping), and Isolation Forest.\n",
    "\n",
    "- Distribution Transformation: Identify skewed data and apply Log Transform or Yeo-Johnson power transformations to improve model performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Implement Feature Engineering & Encoding\n",
    "\n",
    "- Categorical Encoding: Distinguish between and apply OneHotEncoder (for nominal data) and OrdinalEncoder/LabelEncoder (for ordinal/target data).\n",
    "\n",
    "- Feature Scaling: Master the use of StandardScaler (Z-score) and MinMaxScaler, understanding why scaling is critical for many algorithms.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Automate with Scikit-Learn Pipelines\n",
    "\n",
    "- Core Skill: Encapsulate preprocessing steps using ColumnTransformer and Pipeline.\n",
    "\n",
    "- Value: Ensure consistent transformations across training and test sets to effectively prevent Data Leakage and simplify model deployment.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. Optimize Model Selection & Evaluation\n",
    "\n",
    "- Stratified Splitting: Use StratifiedShuffleSplit to ensure train/test set distributions match the overall population, avoiding sampling bias.\n",
    "\n",
    "- Hyperparameter Tuning: Automate the search for the best model parameters (e.g., n_estimators, max_features) using GridSearchCV and RandomizedSearchCV."
   ],
   "id": "a8ea7d5c99e738ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Project Setup & Data Split",
   "id": "df4e572dcc80331"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "95901b53cfa55f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:32.991969Z",
     "start_time": "2025-11-29T07:08:32.983729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "b2a7a58b553d2506",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load and Read Dataset",
   "id": "4b57a41ad570fcdf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:33.885433Z",
     "start_time": "2025-11-29T07:08:33.011252Z"
    }
   },
   "source": [
    "# Url to Dataset\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml2/refs/heads/master/datasets/housing/housing.csv\"\n",
    "\n",
    "# Read the dataset from the url link\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display first row\n",
    "df.head(1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Data Analysis (EDA)",
   "id": "d59e444b7705263c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking the distribution of categorical features",
   "id": "69f568efa5ca6252"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:34.266367Z",
     "start_time": "2025-11-29T07:08:34.257978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"ocean_proximity\"].value_counts()\n",
    "\n",
    "# df[\"ocean_proximity\"].unique() # check distinct categories"
   ],
   "id": "27f8220446578e41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ocean_proximity\n",
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "NEAR OCEAN    2658\n",
       "NEAR BAY      2290\n",
       "ISLAND           5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking for missing values",
   "id": "f0403a615d9f88f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:34.390539Z",
     "start_time": "2025-11-29T07:08:34.379940Z"
    }
   },
   "cell_type": "code",
   "source": "df.isnull().sum()",
   "id": "19d8e0cf2be6373a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude               0\n",
       "latitude                0\n",
       "housing_median_age      0\n",
       "total_rooms             0\n",
       "total_bedrooms        207\n",
       "population              0\n",
       "households              0\n",
       "median_income           0\n",
       "median_house_value      0\n",
       "ocean_proximity         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking for duplicates",
   "id": "90b2a527cbe277dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:34.691989Z",
     "start_time": "2025-11-29T07:08:34.678228Z"
    }
   },
   "cell_type": "code",
   "source": "df.duplicated().sum()  # Check for duplicates",
   "id": "ccdab9ea69e5915",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking Feature Correlations",
   "id": "9a7a54e225959e7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:34.806941Z",
     "start_time": "2025-11-29T07:08:34.795336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corr_matrix = df.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "# corr_matrix #display all correlation between each category"
   ],
   "id": "2d569ade4344bcbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median_house_value    1.000000\n",
       "median_income         0.688075\n",
       "total_rooms           0.134153\n",
       "housing_median_age    0.105623\n",
       "households            0.065843\n",
       "total_bedrooms        0.049686\n",
       "population           -0.024650\n",
       "longitude            -0.045967\n",
       "latitude             -0.144160\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Splitting",
   "id": "2f00bfb07c1cb893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Split and Create the 80% Train and 20% Test Dataset",
   "id": "9837887bd346dbf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.104955Z",
     "start_time": "2025-11-29T07:08:35.083586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='ocean_proximity')\n",
    "y = df['ocean_proximity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,    \n",
    "    random_state=42,  \n",
    "    shuffle=True        \n",
    ")"
   ],
   "id": "d6c096cfc8329c2c",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Train–Validation–Test Split\n",
    "\n",
    "**Train Set** — Used to Learn Patterns\n",
    "\n",
    "**Validation Set** — Used to Tune the Model\n",
    "\n",
    "**Test Set** — Used Once at the Very End"
   ],
   "id": "a97bf7636fcc8fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.341141Z",
     "start_time": "2025-11-29T07:08:35.311032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='ocean_proximity')\n",
    "y = df['ocean_proximity']\n",
    "\n",
    "# Train + Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Temp → Val + Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ],
   "id": "19b6191ae9dc09b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 9) (3096, 9) (3096, 9)\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Stratified Shuffle Split\n",
    "\n",
    "create train/test splits while keeping the same class proportions in each split as in the original dataset."
   ],
   "id": "bcba8588928ce740"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.447933Z",
     "start_time": "2025-11-29T07:08:35.411184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df['ocean_proximity']):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "\n",
    "print(\"Train Set Category Proportions\")\n",
    "print(strat_train_set['ocean_proximity'].value_counts(normalize=True))\n",
    "\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"Test Set Category Proportions\")\n",
    "print(strat_test_set['ocean_proximity'].value_counts(normalize=True))\n",
    "\n",
    "df = strat_train_set.copy()"
   ],
   "id": "6fccc209e27e1fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Category Proportions\n",
      "ocean_proximity\n",
      "<1H OCEAN     0.442622\n",
      "INLAND        0.317414\n",
      "NEAR OCEAN    0.128807\n",
      "NEAR BAY      0.110950\n",
      "ISLAND        0.000208\n",
      "Name: proportion, dtype: float64\n",
      "==============================\n",
      "Test Set Category Proportions\n",
      "ocean_proximity\n",
      "<1H OCEAN     0.442668\n",
      "INLAND        0.317345\n",
      "NEAR OCEAN    0.128714\n",
      "NEAR BAY      0.110950\n",
      "ISLAND        0.000323\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Data Cleaning"
   ],
   "id": "5acad19ce712f8aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Remove duplicates",
   "id": "339a674f35cdccb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.468530Z",
     "start_time": "2025-11-29T07:08:35.454969Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.drop_duplicates()  # Remove these rows",
   "id": "6012f878cc9c89d6",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Handle missing values using imputer",
   "id": "294cb41a80f9ed40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.497744Z",
     "start_time": "2025-11-29T07:08:35.483217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a Simple Imputer\n",
    "imputer = SimpleImputer(strategy='median') # strategy -> 'mean' / 'most_frequent' / 'median' / 'constant'\n",
    "\n",
    "# Fill NAN using median\n",
    "df['total_bedrooms'] = imputer.fit_transform(df[['total_bedrooms']])\n",
    "\n",
    "print(imputer.statistics_)"
   ],
   "id": "886aee80e10559f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[433.]\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Remove rows with missing values",
   "id": "bc74e62760e22d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.550644Z",
     "start_time": "2025-11-29T07:08:35.541204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop rows with empty values\n",
    "df = df.dropna(axis=1)"
   ],
   "id": "58318377877149e9",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Remove outliers using **IQR**",
   "id": "6d8d38a163edde70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.589400Z",
     "start_time": "2025-11-29T07:08:35.576291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# IQR\n",
    "Q1 = df['median_house_value'].quantile(0.25)\n",
    "Q3 = df['median_house_value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['median_house_value'] < lower_bound) | (df['median_house_value'] > upper_bound)]\n",
    "\n",
    "# plt.boxplot(df['median_house_value'])\n",
    "# plt.show()"
   ],
   "id": "5262dab4155687c0",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Capping/Winsorizing\n",
    "\n",
    "Winsoring columns to reduce the impact of extreme outliers"
   ],
   "id": "90f8ba8c4e6b40c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:35.638519Z",
     "start_time": "2025-11-29T07:08:35.627220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lower = df['median_house_value'].quantile(0.01)\n",
    "upper = df['median_house_value'].quantile(0.99)\n",
    "df['median_house_value'] = df['median_house_value'].clip(lower, upper)"
   ],
   "id": "a05b3ee5672d324e",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Isolation Forest\n",
    "\n",
    "To automatically detect outliers in a dataset"
   ],
   "id": "51703ea592af62da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.002641Z",
     "start_time": "2025-11-29T07:08:35.675111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def detect_outliers(df):\n",
    "    #only works with numerical features\n",
    "    num_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "    # Initialize Isolation Forest\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42) # expect about 1% of samples to be anomalies\n",
    "\n",
    "    # # Train the model and let it label which rows are anomalies (-1) or normal (1)\n",
    "    pred = iso.fit_predict(df[num_cols])\n",
    "\n",
    "    # Add the anomaly results to the dataframe\n",
    "    df[\"outlier\"] = (pred == -1).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = detect_outliers(df)\n",
    "df[df[\"outlier\"] == 1].head(2)"
   ],
   "id": "eddf41326a1229bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "9185    -118.56     34.41                 4.0      17313.0          3224.0   \n",
       "9159    -118.45     34.44                16.0      13406.0          2574.0   \n",
       "\n",
       "      population  households  median_income  median_house_value  \\\n",
       "9185      6902.0      2707.0         5.6798            320900.0   \n",
       "9159      7030.0      2440.0         4.6861            187900.0   \n",
       "\n",
       "     ocean_proximity  outlier  \n",
       "9185       <1H OCEAN        1  \n",
       "9159       <1H OCEAN        1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9185</th>\n",
       "      <td>-118.56</td>\n",
       "      <td>34.41</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17313.0</td>\n",
       "      <td>3224.0</td>\n",
       "      <td>6902.0</td>\n",
       "      <td>2707.0</td>\n",
       "      <td>5.6798</td>\n",
       "      <td>320900.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>-118.45</td>\n",
       "      <td>34.44</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13406.0</td>\n",
       "      <td>2574.0</td>\n",
       "      <td>7030.0</td>\n",
       "      <td>2440.0</td>\n",
       "      <td>4.6861</td>\n",
       "      <td>187900.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 183
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Transformation",
   "id": "1aef20dc673fdfda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Log Transform\n",
    "\n",
    "Suitable for: **Right-skewness**\n",
    "\n",
    "- All values are positive"
   ],
   "id": "4989b64866f190ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This step can be used to examine the skewness patterns.",
   "id": "39004ac53b2b41a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.076849Z",
     "start_time": "2025-11-29T07:08:36.072630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# num_cols = df.select_dtypes(include='number').columns\n",
    "#\n",
    "# df[num_cols].hist(figsize=(10, 8), bins=50)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "836e698fe46b5b94",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After that, we choose columns to transform",
   "id": "62ae312705f3f97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.222353Z",
     "start_time": "2025-11-29T07:08:36.206836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_log = df.copy()\n",
    "cols = [\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"]\n",
    "for col in cols:\n",
    "    df_log[col+\"_log\"] = np.log1p(df_log[col])"
   ],
   "id": "a8389f1566422b78",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Yeo-Johnson\n",
    "\n",
    "Automatically finds skewed numerical features in your DataFrame and applies the Yeo-Johnson transformation to reduce the skewness.\n",
    "\n",
    "- Data included zero or negative\n",
    "\n",
    "- mixed skewness"
   ],
   "id": "67e9a2b8a4c833df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.367590Z",
     "start_time": "2025-11-29T07:08:36.290650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def auto_yeojohnson(df, threshold=1.0):\n",
    "    num_cols = df.select_dtypes(include=\"number\").columns\n",
    "    skewed = df[num_cols].skew()\n",
    "    cols = skewed[skewed > threshold].index.tolist()\n",
    "\n",
    "    pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "    df[cols] = pt.fit_transform(df[cols])\n",
    "\n",
    "    print(\"Applied Yeo-Johnson to:\", cols)\n",
    "    return df\n",
    "\n",
    "df3 = auto_yeojohnson(df)"
   ],
   "id": "fa6de218a36bc340",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Yeo-Johnson to: ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'outlier']\n"
     ]
    }
   ],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Encoding",
   "id": "34b110bdf5be5f4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. One Hot Encoder\n",
    "\n",
    "Suitable for: **Feature without order**\n",
    "\n",
    "- Convert unordered categorical features into multiple binary columns (one per category)\n",
    "\n"
   ],
   "id": "61f9465fbe79bd34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.429056Z",
     "start_time": "2025-11-29T07:08:36.399806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform(df[['ocean_proximity']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['ocean_proximity']))\n",
    "df3 = pd.concat([df.drop('ocean_proximity', axis=1), encoded_df], axis=1)\n",
    "\n",
    "df3.head(3)"
   ],
   "id": "ef0b8d967310a41d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "11844    -120.92     40.02                35.0    -2.044276       -1.945714   \n",
       "10949    -117.87     33.75                18.0    -1.424389       -0.772604   \n",
       "17965    -121.99     37.32                20.0     1.101247        1.029851   \n",
       "\n",
       "       population  households  median_income  median_house_value   outlier  \\\n",
       "11844   -2.091243   -2.096325      -0.604136            102500.0 -0.100686   \n",
       "10949   -0.526188   -0.870282      -0.590028            162500.0 -0.100686   \n",
       "17965    0.825399    1.034451       0.649832            217700.0 -0.100686   \n",
       "\n",
       "       ocean_proximity_<1H OCEAN  ocean_proximity_INLAND  \\\n",
       "11844                        0.0                     1.0   \n",
       "10949                        1.0                     0.0   \n",
       "17965                        NaN                     NaN   \n",
       "\n",
       "       ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
       "11844                     0.0                       0.0   \n",
       "10949                     0.0                       0.0   \n",
       "17965                     NaN                       NaN   \n",
       "\n",
       "       ocean_proximity_NEAR OCEAN  \n",
       "11844                         0.0  \n",
       "10949                         0.0  \n",
       "17965                         NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>outlier</th>\n",
       "      <th>ocean_proximity_&lt;1H OCEAN</th>\n",
       "      <th>ocean_proximity_INLAND</th>\n",
       "      <th>ocean_proximity_ISLAND</th>\n",
       "      <th>ocean_proximity_NEAR BAY</th>\n",
       "      <th>ocean_proximity_NEAR OCEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>-120.92</td>\n",
       "      <td>40.02</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-2.044276</td>\n",
       "      <td>-1.945714</td>\n",
       "      <td>-2.091243</td>\n",
       "      <td>-2.096325</td>\n",
       "      <td>-0.604136</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>-0.100686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949</th>\n",
       "      <td>-117.87</td>\n",
       "      <td>33.75</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1.424389</td>\n",
       "      <td>-0.772604</td>\n",
       "      <td>-0.526188</td>\n",
       "      <td>-0.870282</td>\n",
       "      <td>-0.590028</td>\n",
       "      <td>162500.0</td>\n",
       "      <td>-0.100686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17965</th>\n",
       "      <td>-121.99</td>\n",
       "      <td>37.32</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.101247</td>\n",
       "      <td>1.029851</td>\n",
       "      <td>0.825399</td>\n",
       "      <td>1.034451</td>\n",
       "      <td>0.649832</td>\n",
       "      <td>217700.0</td>\n",
       "      <td>-0.100686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Label Encoder\n",
    "\n",
    "Suitable for: **Target Variable**\n",
    "\n",
    "- Convert class labels (strings) into integer codes, typically for target variable (y)"
   ],
   "id": "50cda46f4fb7e272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.500079Z",
     "start_time": "2025-11-29T07:08:36.488210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "df2 = df.copy()\n",
    "df2['ocean_proximity'] = Encoder.fit_transform(df2['ocean_proximity'])\n",
    "\n",
    "df2['ocean_proximity'].value_counts()"
   ],
   "id": "9b55df1b0a9b81e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ocean_proximity\n",
       "0    6395\n",
       "1    4586\n",
       "4    1861\n",
       "3    1603\n",
       "2       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Ordinal Encoder\n",
    "\n",
    "Suitable for: **Feature with order**\n",
    "\n",
    "- Convert ordered categorical features into integer values representing ranking or level"
   ],
   "id": "a443a790cd17c2c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.752384Z",
     "start_time": "2025-11-29T07:08:36.741901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'color': ['red', 'green', 'blue', 'green', 'red'],\n",
    "    'size': ['S', 'M', 'L', 'XL', 'M']\n",
    "})\n",
    "print(df2)\n",
    "\n",
    "print(\"-\"*15)\n",
    "encoder = OrdinalEncoder()\n",
    "df_encoded = df2.copy()\n",
    "df_encoded[['color', 'size']] = encoder.fit_transform(df2[['color', 'size']])\n",
    "\n",
    "print(df_encoded)"
   ],
   "id": "33bd7807e62ce0ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   color size\n",
      "0    red    S\n",
      "1  green    M\n",
      "2   blue    L\n",
      "3  green   XL\n",
      "4    red    M\n",
      "---------------\n",
      "   color  size\n",
      "0    2.0   2.0\n",
      "1    1.0   1.0\n",
      "2    0.0   0.0\n",
      "3    1.0   3.0\n",
      "4    2.0   1.0\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Scaling",
   "id": "e208a0d4ba4317cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Standard Scaler\n",
    "\n",
    "Scales features to have mean = 0 and standard deviation = 1\n",
    "\n",
    "- Sensitive to outliers"
   ],
   "id": "b940114608451f7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.844618Z",
     "start_time": "2025-11-29T07:08:36.833909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "id": "d3438a0e62a1419a",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. MinMax Scaler\n",
    "\n",
    "Scales features to a fixed range, usually [0, 1]\n",
    "\n",
    "- Sensitive to outliers"
   ],
   "id": "6c0301d1dd0dacba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.877113Z",
     "start_time": "2025-11-29T07:08:36.865524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = df.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "id": "eea87224b47ba742",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Robust Scaler\n",
    "\n",
    "Uses median and IQR (Interquartile Range)\n",
    "\n",
    "- Data has outliers or is skewed\n",
    "- Keeps most values within a manageable range without being affected by extreme values."
   ],
   "id": "ec87511dcdec46b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.911084Z",
     "start_time": "2025-11-29T07:08:36.895684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X = df.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "id": "fe9b601c5f0b0708",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipelines\n",
    "\n",
    "The pipeline combines preprocessing and model training into a single, unified workflow.\n",
    "\n",
    "**Remind**: The steps must be executed in the correct order."
   ],
   "id": "512a60470dfffdcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:08:36.931602Z",
     "start_time": "2025-11-29T07:08:36.922621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # features (X)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # target (y)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_cols = housing.select_dtypes(include='number').columns\n",
    "cat_cols = housing.select_dtypes(exclude='number').columns\n",
    "\n",
    "# Preprocessing for numerical features:\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical features:\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "# so each column receives the appropriate preprocessing.\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline:\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ],
   "id": "14e55a93a6d4ddff",
   "outputs": [],
   "execution_count": 193
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training & Hyperparameter Tuning",
   "id": "a3cc3d89f2b9fc37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Grid Search",
   "id": "e068db8fdd07b46d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:10:31.571003Z",
     "start_time": "2025-11-29T07:08:36.945280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'model__n_estimators': [30, 100],\n",
    "    'model__max_features': [2, 4, 8],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Train using the training set (housing & housing_labels)\n",
    "grid.fit(housing, housing_labels)\n",
    "\n",
    "# Output the best-found hyperparameters\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Convert the negative MSE back to a readable RMSE\n",
    "best_rmse = np.sqrt(-grid.best_score_)\n",
    "print(\"Best Cross-Validation Score (RMSE):\", best_rmse)\n",
    "\n",
    "# Extract features and labels from the stratified test set\n",
    "X_test_final = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test_final = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Retrieve the best model found by GridSearchCV\n",
    "best_model = grid.best_estimator_\n",
    "final_predictions = best_model.predict(X_test_final)\n",
    "\n",
    "# Compute the final RMSE to measure real-world performanceE\n",
    "final_mse = mean_squared_error(y_test_final, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) # <--- 修正点3：计算 RMSE\n",
    "print(\"Final RMSE:\", final_rmse)"
   ],
   "id": "882be19464bb6f75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__max_features': 8, 'model__n_estimators': 100}\n",
      "Best Cross-Validation Score (RMSE): 49684.84664464555\n",
      "Final RMSE: 48244.37018042808\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:35:21.849276Z",
     "start_time": "2025-11-29T07:35:21.817177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the best model\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "# Get the feature importance values\n",
    "feature_importances = final_model.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "# Since you used a Pipeline (OneHotEncoder), the number of feature names increased.\n",
    "# We need to align them. Here we simplify by only looking at the numerical values,\n",
    "# or you can try to manually align the column names.\n",
    "\n",
    "print(\"Feature importance ranking:\")\n",
    "\n",
    "# Assume 'attributes' is your list of feature names.\n",
    "# (If it’s inconvenient to get the column names, you can directly print the values to see the distribution.)\n",
    "# sorted(zip(feature_importances, attributes), reverse=True)\n",
    "\n",
    "print(feature_importances)"
   ],
   "id": "60c974ccd0be8584",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance ranking:\n",
      "[1.13307588e-01 1.03569127e-01 4.90404095e-02 2.99481958e-02\n",
      " 2.79803109e-02 3.79115572e-02 2.56946273e-02 4.38899323e-01\n",
      " 1.03560399e-02 1.54348407e-01 3.45530037e-04 2.20241299e-03\n",
      " 6.39647136e-03]\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Randomized Search",
   "id": "21dd2d7fcb3261d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T07:10:35.520606Z",
     "start_time": "2025-11-29T07:10:32.258034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 2. Train/Test Split\n",
    "# Splits the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Initialize a RandomForest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 4. Define \"range-based\" hyperparameter distributions (key concept of RandomizedSearchCV)\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),      # Randomly sample between 50 and 300 trees\n",
    "    'max_depth': randint(3, 20),           # Randomly sample tree depth between 3 and 20\n",
    "    'min_samples_split': randint(2, 10),   # Randomly sample minimum split size between 2 and 10\n",
    "}\n",
    "\n",
    "# 5. Set up Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,            # Draw 20 random combinations (not all)\n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    scoring='accuracy',   # Evaluate using accuracy\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# 6. Train the model (Random sampling + cross-validation on training set)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Display the best-found hyperparameters and CV score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", random_search.best_score_)\n",
    "\n",
    "# 8. Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))"
   ],
   "id": "107f7c81f983c701",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 9, 'min_samples_split': 5, 'n_estimators': 142}\n",
      "Best Cross-Validation Score: 0.95\n",
      "Test Set Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 195
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
